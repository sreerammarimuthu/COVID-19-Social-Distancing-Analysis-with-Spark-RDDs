{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29cfb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1- Spark-RDDs: Scalable Covid19-related Applications\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from faker import Faker\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import csv\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Creating a SparkConf with memory configurations to handle the big data\n",
    "conf = SparkConf().setAppName(\"Covid\").set(\"spark.executor.memory\", \"4g\").set(\"spark.driver.memory\", \"4g\").set(\"spark.network.timeout\", \"600s\")\n",
    "\n",
    "# Creating a SparkContext\n",
    "spark = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# STEP 1 - DATA CREATION\n",
    "\n",
    "def generate_concert_data(name,size):\n",
    "    fake = Faker()\n",
    "    data = []\n",
    "    used_coordinates = set()\n",
    "\n",
    "    for id in range(1, size+1):\n",
    "        # Generate coordiates of the person X and Y values\n",
    "        x = round(random.uniform(1, 10000))\n",
    "        y = round(random.uniform(1, 10000))\n",
    "\n",
    "        # Ensure uniqueness by checking and regenerating if necessary\n",
    "        while (x, y) in used_coordinates:\n",
    "            x = round(random.uniform(1, 10000))\n",
    "            y = round(random.uniform(1, 10000))\n",
    "\n",
    "        # Add the coordinates to the used set\n",
    "        used_coordinates.add((x, y))\n",
    "\n",
    "        data.append({\n",
    "            \"ID\": id,\n",
    "            \"X\": x,\n",
    "            \"Y\": y,\n",
    "            \"Name\": fake.name(),\n",
    "            \"Age\": random.randint(10, 100)\n",
    "        })        \n",
    "    pd.DataFrame(data).to_csv(f\"{name}.csv\", index=False)\n",
    "generate_concert_data('PEOPLE_large', 100000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Storing the created people data in csv file\n",
    "people_large=pd.read_csv('PEOPLE_large.csv')\n",
    "\n",
    "\n",
    "# Creating and storing the data for infected small dataset\n",
    "infected_small=people_large.sample(5000).reset_index(drop=True)\n",
    "infected_small.to_csv('INFECTED_small.csv',index=False)\n",
    "\n",
    "\n",
    "# Creating and storing the data for People some infected large dataset\n",
    "some_infected=pd.read_csv('PEOPLE_large.csv')\n",
    "some_infected['INFECTED']=['Yes' if i in list(infected_small['ID']) else 'No' for i in some_infected['ID']]\n",
    "some_infected=some_infected.reset_index(drop=True)\n",
    "some_infected.to_csv('SOME_INFECTED_large.csv')\n",
    "\n",
    "# Storing the data as RDDs\n",
    "infected_rdd = spark.textFile(\"INFECTED_small.csv\")\n",
    "person_rdd=spark.textFile(\"PEOPLE_large.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66f0dce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2 - QUERIES\n",
    "\n",
    "\n",
    "# Q1 - People from PEOPLE-large that had close contact with an infected person from Infected\n",
    "\n",
    "def csv_(line):\n",
    "    return line.split(',')\n",
    "\n",
    "# Calculating the distance between 2 people\n",
    "def distance_between(point1, point2):\n",
    "    x1, y1 = point1[1], point1[2]\n",
    "    x2, y2 = point2[1], point2[2]\n",
    "    return  (((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5)\n",
    "\n",
    "infected_q1 = infected_rdd.map(csv_).filter(lambda x: x[0] != 'ID')\n",
    "people_q1 = person_rdd.map(csv_).filter(lambda x: x[0] != 'ID')\n",
    "\n",
    "infected_coordinates = infected_q1.map(lambda x: (x[0], float(x[1]), float(x[2]),x[3],x[4]))\n",
    "people_coordinates = people_q1.map(lambda x: ((x[0]), float(x[1]), float(x[2]),x[3],x[4]))\n",
    "\n",
    "infected_id=infected_coordinates.map(lambda x:x[0]).collect()\n",
    "people_coordinates=people_coordinates.filter(lambda x:x[0] not in infected_id)\n",
    "\n",
    "close_contacts=infected_coordinates.cartesian(people_coordinates)\n",
    "close_contacts=close_contacts.filter(lambda pair: distance_between(pair[0], pair[1]) <= 6) \\\n",
    "    .map(lambda pair: (pair[1][0], pair[0][0]))\n",
    "\n",
    "# Printing output of query 1 to a text file\n",
    "#print(close_contacts.collect())\n",
    "\n",
    "# Collecting the results to the driver\n",
    "result_list = close_contacts.collect()\n",
    "\n",
    "output_file_path_q1 = \"Query_1_Output.txt\"\n",
    "\n",
    "# Writing output to the text file\n",
    "with open(output_file_path_q1, \"w\") as file:\n",
    "    for pair in result_list:\n",
    "        file.write(f\"{pair[0]}, {pair[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325c67fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 - Distinct people of Q1 case \n",
    "\n",
    "infected_q2 = infected_rdd.map(csv_).filter(lambda x: x[0] != 'ID')\n",
    "people_q2 = person_rdd.map(csv_).filter(lambda x: x[0] != 'ID')\n",
    "\n",
    "infected_coordinates = infected_q2.map(lambda x: (x[0], float(x[1]), float(x[2]), x[3], x[4]))\n",
    "people_coordinates = people_q2.map(lambda x: (x[0], float(x[1]), float(x[2]), x[3], x[4]))\n",
    "\n",
    "infected_id = infected_coordinates.map(lambda x: x[0]).collect()\n",
    "people_coordinates = people_coordinates.filter(lambda x: x[0] not in infected_id)\n",
    "\n",
    "distinct_close_contacts = infected_coordinates.cartesian(people_coordinates)\n",
    "\n",
    "distinct_close_contacts = distinct_close_contacts.filter(lambda pair: distance_between(pair[0], pair[1]) <= 6) \\\n",
    "    .map(lambda pair: (pair[1][0], pair[0][0])) \\\n",
    "    .map(lambda x: (x[0], x)) \\\n",
    "    .reduceByKey(lambda x, y: x) \\\n",
    "    .map(lambda x: x[1])\n",
    "\n",
    "# Printing output of query 2 to a text file\n",
    "# print(distinct_close_contacts.collect())\n",
    "\n",
    "# Collecting the results to the driver\n",
    "distinct_result_list = distinct_close_contacts.collect()\n",
    "\n",
    "output_file_path_q2 = \"Query_2_Output.txt\"\n",
    "\n",
    "# Writing output to the text file\n",
    "with open(output_file_path_q2, \"w\") as file:\n",
    "    for pair in distinct_result_list:\n",
    "        file.write(f\"{pair[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2887d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 - Infected People with number of people they were in close contact with\n",
    "\n",
    "\n",
    "some_infected_rdd = spark.textFile(\"SOME_INFECTED_large.csv\")\n",
    "some_infected_rdd = some_infected_rdd.map(csv_).filter(lambda x: x[0] != 'ID').filter(lambda x: x[0] != '')\n",
    "some_infected_rdd = some_infected_rdd.map(lambda x: (x[1], float(x[2]), float(x[3]),x[4],x[5],x[6]))\n",
    "\n",
    "people_infected_yes=some_infected_rdd.filter(lambda x:x[5]=='Yes')\n",
    "people_infected_no=some_infected_rdd.filter(lambda x:x[5]=='No')\n",
    "\n",
    "close_contacts_cross=people_infected_yes.cartesian(people_infected_no)\n",
    "\n",
    "close_contacts_cross=close_contacts_cross.filter(lambda pair: distance_between(pair[0], pair[1]) <= 6) \\\n",
    "    .map(lambda pair: (pair[1][0],pair[1][3], pair[0][0],pair[0][3]))\n",
    "\n",
    "infected_counts = close_contacts_cross.map(lambda x: (x[2], 1)).reduceByKey(lambda x, y: x + y).collect()\n",
    "\n",
    "\n",
    "\n",
    "# Printing output of query 3 to a text file\n",
    "# print(infected_counts)\n",
    "\n",
    "# Collecting the results to the driver\n",
    "result_count = infected_counts\n",
    "\n",
    "output_file_path_q3 = \"Query_3_Output.txt\"\n",
    "\n",
    "# Writing output to the text file\n",
    "with open(output_file_path_q3, \"w\") as file:\n",
    "    for pair in result_count:\n",
    "        file.write(f\"{pair[0]}, {pair[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a9f7f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ending the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7252d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
